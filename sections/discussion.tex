\chapter{Discussion} \label{ch:discussion}
% "results indicate that..."
  Examining the trajectories of each of the pigeon models' heads as seen in Figure \ref{fig:head_bs_0}, the resulting behavior exhibited by the pigeon model with a fixed body, whose deep reinforcement learning controller was trained on $r_{fifty\_fifty}$, indicate that the combination of visual stabilization and motion parallax are sufficient to generate a behavior that fixes the position of the head, resembling the hold phase in pigeons.

  On the other hand, the resulting behavior exhibited by the pigeon model with body speed 1, whose deep reinforcement learning controller was trained on the same reward function as seen in Figures \ref{fig:head_bs_1_all_trimmed} and \ref{fig:head_bs_1_closest_trimmed}, indicate that the 2 functionalities are insufficient to produce head-bobbing behaviors in moving bodies during forward locomotion, as he pigeon model did not need to periodically thrust its head forward to maximize Davies' equation depicting motion parallax between objects.

  Such results indicate that visual stabilization with retinal cells capable of detecting movement in all directions is enough to maximize the sum of external objects' angular velocities within the retina.
    Examining Davies' equation \ref{equ:davies_motion_parallax}, it can be hypothesized that reinforcement learning controllers trained on reward function that reflect such function would only reproduce head-bobbing behaviors when all objects within the retina are globally static, since the equation does not account for the objects' velocities.
    Since the external objects in our experiment had objects moving forwards and backwards, such may have automatically increased the reward function depicting Davies' equation of motion parallax per timestep.


% fifty_fifty
  One possible argument against our claim can be pointed to our decision to weigh $r_{head\_stabilize}$ and $r_{motion\_parallax}$ equally for the reward that the policy or controller representing preliminary hypotheses $r_{fifty\_fifty}$. One could argue that by changing the weights, it may be possible to produce different behaviors than those shown in the results.
  Specifically, by assigning a larger weight value for $r_{head\_stabilize}$ than that for $r_{motion\_parallax}$, one could expect a pigeon model with the body speed of 1 to reflect the task of constricting the global position of its head and produce hold phases while occasionally moving the head to induce motion parallax.
  However, considering that the maximum return for $r_{head\_stabilize}$ is 0 and the pigeon whose controller was trained on $r_{fifty\_fifty}$ still managed to produce high positive returns (Figure \ref{fig:learning_rate_fifty_fifty_bs_1}) , it can be inferred that penalizing movement of the head would not change the resulting behavior.
  Therefore, factors other than static visual stabilization $r_{head\_stabilize}$ and motion parallax induction $r_{motion\_parallax}$ are most likely also contributing to the induction of head-bobbing behaviors.

% behavior in bs=1; fifty_fifty
  We examine the biggest behavioral difference between pigeon models in the baseline experiments and their counterparts reflecting preliminary hypotheses, particularly, those trained on $r_{fifty\_fifty}$ with the body speed of 1.
  The latter example, unlike the former, presents a behavior where the head is moved downwards throughout the duration of the experiment (Figure \ref{fig:fifty_fifty_body_speed_1}).
  Despite the major differences between the two experiments, the latter does not suffer from an inability to acquire rewards (Figure \ref{fig:learning_rate_fifty_fifty_bs_1}).
  Considering that constantly bending one's neck downwards outside of its neutral posture would likely result in physical trauma for pigeons, it is possible to suspect that muscular strain may be a motivation to avoid reproducing the behavior seen in our experiment with $r_{fifty\_fifty}$ and adjust their neck posture to maintain an upright position.
  Additionally, muscular simulation and their placements within the physical model may lead to more stabilization in its movements, as seen in Geijtenbeek's simulations of models of bipedal virtual creatures with muscular simulations \cite{geijtenbeek2013flexible}, where an optimization of placements of muscles using covariance matrix adaptation evolution strategy (CMA-ES) resulted in higher stability in the models' locomotion.
  Following such observation, we can hypothesize that a lack of muscular strain penalty in our pigeon model may be the cause in the discrepancies. As a progression in incremental modeling, an addition of muscular physics would be appropriate.


% additional detail to add in our incremental modeling process is a hierarchical control system.
  The lack of emergent behaviors that resemble phases in pigeon models controlled by non-hierarchical reinforcement learning policies trained on $r_{fifty\_fifty}$, indicates that a hierarchical control system may be embedded in pigeons' neurology that activates when they exhibit head-bobbing behaviors.
  An addition of a high-level state machine-like control mechanism composed of a "hold phase" state and a "thrust phase" state may be a viable approach for replicating such behaviors.
  The high-level controller can output values that indicate the appropriate phase for the pigeon model to execute, such as a binary digit where 1 represents the hold phase and 0 represents the thrust phase.

  Hierarchical reinforcement learning could be used for modeling such control systems.
  Preliminary research have examples of successfully accomplishing complex tasks that require sequential execution of multiple smaller tasks.
  Diversity Is All You Need \cite{eysenbach2018diversity} can execute sequential tasks such as running, jumping over hurdles, and resuming running, by acquiring skills, or primitive tasks, in the low-level policy and executing them in order using the high-level policy.
  Similarly, FeUdal Networks \cite{vezhnevets2017feudal} have shown to complete Montezuma's Revenge, an ATARI game that was deemed to be difficult to complete without using control systems that can execute sequential tasks.

% pattern generating module or functionality, such as central pattern generators seen in the spinal cortex
