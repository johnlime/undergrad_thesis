\chapter{Experiments}

% dimensions of the pigeon model
  % Our pigeon model's dimensions and orientations are set at static values for all experiments, as shown in [Figure ???].
    % insert diagram
  % Additionally, the pigeon's head relative to the body is facing the negative direction relative to the x-axis.
  % The widths and heights of each limb, head, and body are $(10, 4)$, $(6, 4)$, $(20, 10)$ respectively.
  % The initial angles of each limb are oriented at 45 degrees relative to the x-axis, and both the head and the body are oriented parallel to the x axis.
  % The body's initial position is at the origin, and is set to move at a constant speed in the negative direction along the x-axis.

% head trajectory setting
  % The target head location $T$ is set at $(0, -2)$ relative to the initial position of the head.
  % The threshold value for the distance between the target position and the torso's position is set at 10.
  % We set a value that represents the margin of error between $T$ and the position of the head, which allows positive reward to be returned from the environment if the position of the head is within it.

% external objects
  % 3 points and their positions are defined to represent 1 static and 2 dynamic objects placed on the surrounding environment of the pigeon.
  % The static object's position is $(-30.0, 30.0)$, and the 2 dynamic objects' positions are $(-30.0, 60.0)$, $(-60.0, 30.0)$. The former dynamic object moves at speed 1 in the positive direction along the x-axis, while the latter moves at the same speed in the negative direction along the x-axis.

% experiments
  % _head_stable_manual_reposition
  % _head_stable_manual_reposition_strict_angle
  % _retinal_stabilization
  % _motion_parallax
  % _fifty_fifty

% We constructed OpenAI Gym environments $PigeonEnv3Joints$ and $PigeonRetinalEnv$ for conducting reinforcement learning based on the baseline training and preliminary hypotheses, respectively.
  % Details regarding the environments' code can be referred to the Appendix.

% SAC a type of reinforcement learning model
  % epochs
  % timesteps
% why we didn't use PPO
  % Baseline for many rl experiments
    % https://scholar.google.com/scholar?as_ylo=2018&q=proximal+policy+optimization&hl=en&as_sdt=0,5
    % https://arxiv.org/abs/1905.01360 (pommerman)
  % we tested it, and SAC seemed to have a more stable learning curve; thought that it would be more reliable
  % SAC represents the exploration of new trajectories of actions more; exploration of better execution seen in biological organisms' behaviors
